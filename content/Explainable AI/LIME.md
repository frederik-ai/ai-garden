["Why Should I Trust You?": Explaining the Predictions of Any Classifier](https://arxiv.org/abs/1602.04938)

**LIME**

The main problem LIME tries to solve is that we can only trust a model if we understand its predictions. LIME therefore creates a model for each prediction that tries to explain which features mostly contributed to this answer.

The abbreviation LIME stands for:
==L==ocal :arrow_right: Explain a single prediction
==I==nterpretable :arrow_right: The goal of LIME is to make its explanation humanly understandable
==M==odel-agnostic :arrow_right: Can be applied to any machine Learning model
==E==xplanations


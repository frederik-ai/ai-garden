In Transfer Learning we try to take a trained model and fit it to a different task. Suppose for example we have a model that is very well at detecting motorbikes in an image. Now for a different project we need to detect bicycles. We could just use the motorbike model, freeze most of its layers and retrain the classification head on images of bicycles. Freezing a layer means to make its parameter untrainable. The idea is that most layers responsible for feature extraction can already detect features that are also important for bicycles. We do not have to retrain those layers. This speeds up training and can also decrease the amount of new training data we need.

We can also use this approach to transfer our model to a very different task. For example if we assume that the features extracted by the model apply universally. An example is that we can take a model trained on ImageNet and then apply it to some very specific classification task that is not at all modeled by ImageNet dataset. This is also used for large language models where we assume that the model has learned to understand and produce language so well that it can transfer this knowledge to instead be used to summarize text or to do text classification.
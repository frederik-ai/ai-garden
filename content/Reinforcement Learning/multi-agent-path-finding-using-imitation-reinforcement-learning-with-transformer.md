---
title: "Multi Agent Path Finding Using Imitation Reinforcement Learning With Transformer"
---

In this paper the authors work in an environment where multiple robots should simulateously work on a path-planning task. The goal is that these robots coordinate their movements without communicating with each other

# Learning Framework

The paper uses Q-Learning to achieve this task. For each state $s_t$ we there are multiple possible actions. One action per robot. Because this leads to an unfeasable Q-Table, the paper uses a deep neural network to learn the optimal Q-Function.

## Model Input

Each robot learns its own Q-function. Somehow the world state has to be encoded into a tensor that can be input into our model. Other robots are viewed as part of the observable environment. Once the environmet is encoded into a tensor it has the following properties:

- The tensor has six channels. Each channel has the size $l\times l$ whereas $l\times l$ is the field of view of the model.
- The first channel contains the position of all other robots. In the $l \times l$ grid each cell where there is a robot is marked with 1. All other cells are 0.
- The second channel is the same as the first, but the locations of the obstacles instead of other robots are marked with a 1.
- The four other channels are called “heuristics channels”. The authors take the idea from [this][heuristics_publication] publication. The idea is simple:
  - Each robot uses a simple RL-function to learn all the shortest paths to the goal
  - Each of the four channels corresponds to the action up, left, right or down
  - A grid cell in the channel has the value 1 if the robot gets closer to the goal when being at the location of the grid cell and doing the action of the channel

## Model

The authors use the transformer architecture.

In order to feed the input into the transformer they do the following: They divide the 9x9x6 input tensor into nine 3x3x6 inputs. These are then fed into the transformer as the *sequence*. With transformers they can use positional encodings. This is important because each of these 3x3x6 inputs corresponds to a direction (front, left, front left, ...). We also have to convert each of these tensors into an embedding (vector of some dimension $D$). Therefore the authors apply a Multi-Layer-Percetron to each of these tensors.

## Imitation Learning

The authors use imitation learning to provide the model with optimal examples it should learn to follow. The “labeled” expert examples are generated by an algorithm called ODrM*.

## Optimization

The authors use two loss-function. One of them measures the performance compared to the expert system for imitation learning. This is called **Host Supervision Contrastive Loss**. The other one is **Double Deep Q-learning Loss**.

### Double Deep Q-learning Loss

What we basically want to learn is a Q-function. If this Q-function is optimal, we can trust that if we take the action with the highest Q-value (out of all possible state-action pairs), we will maximize future rewards.

#### Normal Deep Q-Learning (DQN) Loss

Given a previous iteration of Q-values ($i-1$), our ground truth Q-value for a given state-action pair is:



# References

Distributed Heuristic Multi-Agent Path Finding with Communication (https://ieeexplore.ieee.org/document/9560748)

ODrM* optimal multirobot path planning in low dimensional search spaces ([https://ieeexplore.ieee.org/document/6631119](https://ieeexplore.ieee.org/document/6631119))

